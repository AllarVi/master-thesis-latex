\section{Related work}


% 
% Background
% 

% Parkinson's disease (PD) is very complex neurodegenerative disorder which mostly affects human motions. Patients demonstrate variety of symptoms such as tremor, rigidity and slowness in movements(bradykinesia) \cite{moustafa2016motor, smits2014standardized, shukla2012micrographia, drotar2016evaluation}. Handwriting and drawing processes are complex fine motor activities, which require precise coordination of many muscles, hence these processes are vulnerable among PD patients. Recent studies\cite{nackaerts2017validity, letanneux2014micrographia, drotar2015decision} suggest drawing and handwriting analysis as a biomarker for PD diagnosis. 

% 
% Data acquisition
% 


% 
% Test types
% 

% \citet{nomm2016quantitative} investigates kinematic parameters of Luria's tests, which consist of a number of different drawing exercises, requiring different complexity of the motion planning. This allows to differentiate impairments on the different levels related to motion planning and execution. 

% 
% Features
% 

% 
% Link with recent works
% 

% 
% Interpretation of black-box models
% 

% Lime

% Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.

% In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.

% In this paper, we argued that trust is crucial for effective human interaction with machine learning systems, and that explaining individual predictions is important in assessing trust. We proposed LIME, a modular and extensible ap- proach to faithfully explain the predictions of any model in an interpretable manner. We also introduced SP-LIME, a method to select representative and non-redundant predictions, providing a global view of the model to users. Our experiments demonstrated that explanations are useful for a variety of models in trust-related tasks in the text and image domains, with both expert and non-expert users: deciding between models, assessing trust, improving untrustworthy models, and getting insights into predictions.



% RF

% Model interpretation is one of the key aspects of the model evaluation process. The explanation of the relationship between model variables and outputs is relatively easy for statistical models, such as linear regressions, thanks to the availability of model parameters and their statistical signif- icance. For “black box” models, such as random forest, this information is hidden inside the model structure. This work presents an approach for computing feature contributions for random forest classification models. It allows for the determination of the influence of each variable on the model prediction for an individual instance. By analysing feature contributions for a training dataset, the most significant variables can be determined and their typical contribution towards predictions made for individual classes, i.e., class-specific feature contribution ”patterns”, are discovered. These patterns represent a standard behaviour of the model and allow for an additional as- sessment of the model reliability for a new data. Interpretation of feature contributions for two UCI benchmark datasets shows the potential of the proposed methodology. The robustness of results is demonstrated through an extensive analysis of feature contributions calculated for a large number of generated random forest models.