\begin{comment}

Keywords:

    - The kind of data needed
    - Data example
    - Equipment
    - Filming process
    - Video to keypoints data conversion
    - Pose estimation
    - Software
    - Hardware
    - Process
    - Data produced and quality control
        

\end{comment}

In order to combine gymnastics activity recognition and computer vision, the data acquisition phase of this thesis begins by collecting the recordings of athletes performing gymnastic activities. The activities chosen are the backflip (section \ref{backflip-description-section}) and the back handspring (section \ref{back-handspring-description-section}). The motivation behind the chosen activities is that these activities are some of the foundation exercises that athletes use as building blocks for more difficult combinations. These activities are particularly interesting since even seasoned athletes use them in their training regime and try to polish their technique throughout their whole career. The recordings are collected by a consumer grade action camera (section \ref{the-recordings-process-and-results}). After recording the activities, human skeleton data will be extracted from the videos using computer vision's technique called \textit{pose estimation}, an important step to transform the data from video format to a data format more suitable for training machine learning models to recognize the activities.

The data to be explored and used to train machine learning models in this thesis plays central part in our implementation. Although, machine learning promises to loosen the strictness of the data used in a system by trying to generalize and adapt the models themselves to the data, the quality of the initial data used to develop prototypes still directly influences the interpretation and value of the outcome. Interpretation of not only the outcome, but the entire solution is needed to have clear understanding of why certain outcomes exist, which in turn helps to spark discussion and spread knowledge gained during an experiment. The value of this research, reusable by peers for new scientific experiments, is also directly influenced by the quality of the initial data. 

% The author of this work is aware, that by choosing a very niche dataset he risks to reduce the social reach of this work and thereby stresses to point that he mainly wishes to contribute his finding to the human activity recognition domain and not necessarily to the specific sports domain of gymnastics.

The data acquisition chapter is split into two parts. The first part (\ref{recording-human-actions}) explains what kind and how much of data the author is collecting and the process behind it. The second part (\ref{estimating-and-collecting-human-action-poses}) explains a more technical approach on how the initial data for the human activity recognition algorithm is acquired from recorded human motions and the tools used to do so.

\section{Recording Human Actions}
\label{recording-human-actions}

\subsection{Choosing Activities To Record}

When it comes to choosing difficult biomechanical activities performed by humans, gymnastics is one on the top of the list. Gymnastic feats don't require any additional equipment by humans, but at the same time physical strength, flexibility and kinesthetic awareness are a must to perform any of the skills demonstrated by elite athletes. Gymnastics is also special in its indisputable need for utilizing every part of the human body. All major muscle groups of an athlete need to be in superior condition to perform certain rotations, jumps and holds. As age is a limiting factor when it comes to peak physical condition, many athletes start their career at a very young age. However, there is no age limit for practicing gymnastics at a recreational level.

Working with gymnastic movements and trying to automate the recognition of this kind of human activity requires a reference or in machine learning terms \textit{training data} to train our machine. The two activities chosen to prove the hypothesis of this paper are known as backflips and back handsprings. While fast and explosive, these activities are similar in their execution and achievable by all levels of gymnastics trainees. As backflips and back handsprings are the building blocks of more complex gymnastic combinations, perfecting the technique of these activities provides athletes with the confidence and skill necessary to move on to more difficult feats. From the need for perfecting the technique of these basic activities comes the motivation to analyze them and automate the feedback loop for the athletes. From the athlete's perspective, it is faster and cheaper to get the feedback from, for example, a portable device with recording capabilities, rather than a gymnastics coach. From the coach's perspective, it is also more convenient to automate the learning process of easier activities so the coach can concentrate on guiding athletes towards more interesting and challenging exercises. Figures \ref{example-of-backflip} and \ref{example-of-back-handspring} will provide the reader a visual idea of how backflips and back handsprings are performed respectively. 

\begin{figure*}
   \centering
\begin{tabular}{ccc}
\includegraphics[width=5cm]{images/data-acquisition/example-backflip-part-1}&
\includegraphics[width=5cm]{images/data-acquisition/example-backflip-part-2}&
\includegraphics[width=5cm]{images/data-acquisition/example-backflip-part-3}\\
\includegraphics[width=5cm]{images/data-acquisition/example-backflip-part-4}&
\includegraphics[width=5cm]{images/data-acquisition/example-backflip-part-5}&
\includegraphics[width=5cm]{images/data-acquisition/example-backflip-part-6}\\
\end{tabular}
    \caption{Example of a backflip}
    \label{example-of-backflip}
\end{figure*}

\begin{figure*}
   \centering
\begin{tabular}{ccc}
\includegraphics[width=5cm]{images/data-acquisition/example-flack-part-1}&
\includegraphics[width=5cm]{images/data-acquisition/example-flack-part-2}&
\includegraphics[width=5cm]{images/data-acquisition/example-flack-part-3}\\
\includegraphics[width=5cm]{images/data-acquisition/example-flack-part-4}&
\includegraphics[width=5cm]{images/data-acquisition/example-flack-part-5}&
\includegraphics[width=5cm]{images/data-acquisition/example-flack-part-6}\\
\end{tabular}
    \caption{Example of a back handspring}
    \label{example-of-back-handspring}
\end{figure*}

\subsection{Identifying Activities}

As with all identifiable activities, it should be clear when the activity starts and when it ends. This paper proposes specific start and end markers for both activities, so they could be easily recognized and validated by a human observer. The markers should cover the full duration of a movement, all while sustaining the integrity and focusing on the period of interest of each activity.

A more detailed explanation of each activity and references to the visual markers can be found in sections \ref{backflip-description-section} and \ref{back-handspring-description-section}.

\subsection{Backflip}
\label{backflip-description-section}

A backflip is a sequence of body movements in which a person leaps into the air and rotates backwards over the body's horizontal axis. For the backflip, we mark the start of a backflip as the frame when athlete's both arms pass the horizontal line at shoulder level moving downwards and generating momentum. We mark the end of a backflip as the frame when both heels of the feet touch the ground again. We choose the heels of feet so we can include the amortization part of the landing phase of the backflip in the recording. The red bars in figure \ref{example-backflip-markers} demonstrate the visual markers for trimming the sample recording to include only the activity under investigation. These markers were chosen by the author to the best of his knowledge of the domain as the clearest points for a human observer to recognize a backflip. Choosing different markers is a potential discussion topic for future improvements.

\begin{figure*}
   \centering
\begin{tabular}{cc}
\includegraphics[width=7cm]{images/data-acquisition/example-backflip-marker-start}&
\includegraphics[width=7cm]{images/data-acquisition/example-backflip-marker-end}\\
\end{tabular}
    \caption{Start and end markers for covering the full duration of a backflip}
    \label{example-backflip-markers}
\end{figure*}

\subsection{Back Handspring}
\label{back-handspring-description-section}

A back handspring is similar to a backflip in that the athlete also rotates his body around the horizontal axis. However, during a back handspring, the athlete also moves backwards, while during a backflip the athlete should ideally not move in any direction. The other clear distinction between a back handspring and backflip is that during a back handspring, the athlete's arms extend and push off the floor to create the spring part of the activity and keep the athlete moving backwards. Thirdly, the takeoff angle differs for both back handspring and backflip. The takeoff angle decides in which direction the athlete moves during the rotation. For the backflip investigated in this paper, the goal of the athletes was to keep the takeoff angle as perpendicular as possible to the floor to keep the athlete from moving either backwards or forwards during the rotation. However, for the back handspring, the takeoff angle should be around 45$^{\circ}$ backwards to the floor, to help the athlete move backwards and land on the hands. The green bars in figure \ref{example-back-handspring-markers} demonstrate the visual markers for covering the full duration of a back handspring. The color of markers are in both cases irrelevant and are chosen primarily for better visual distinction.

\begin{figure*}
   \centering
\begin{tabular}{cc}
\includegraphics[width=7cm]{images/data-acquisition/example-flack-marker-start}&
\includegraphics[width=7cm]{images/data-acquisition/example-flack-marker-end}\\
\end{tabular}
    \caption{Start and end markers for covering the full duration of a back handspring}
    \label{example-back-handspring-markers}
\end{figure*}

\subsection{The Recording Process and Results}
\label{the-recordings-process-and-results}

Since the types of movements under investigation are performed for a duration of time, just still images of activities are not sufficient. We need the activities recorded in some kind of motion picture format. The device used to capture activities for this paper is a GoPro Hero7 Black, with the following basic settings:

\begin{easylist}[itemize]

& \textit{RES (resolution)} --- 1080p
& \textit{FPS (frame rate)} --- 60
& \textit{FOV (field of view)} --- Linear
& \textit{Low Light} --- Auto
& \textit{Stabilization} --- Auto
& \textit{Protune} --- Off

\end{easylist}


Each activity is recorded as one still view from eye level fully showing the subjects body from head to toe. The \textit{three-quarter view} (\textit{two-thirds view}) is chosen to capture each activity. Choosing this angle avoids the limbs of the subject stacking behind each other in the recording. Similar works in this domain use either sophisticated three-dimensional motion capture systems using six cameras with depth sensors \cite{kinematic-back-handspring-analysis} or older works using bigger cameras and recording perpendicular to the plane of the movement \cite{Burgess2001KINEMATICAO}. While very thorough analyzes, taking into account the center of body mass or exact takeoff and landing angles, they are harder to reuse in a wider variety of difficult biomechanical movements. Mentioned works often require athletes to wear special gear (markers) and also require longer setup times for the recording cameras. The author of this work tries to emphasize on the potential for improving \textit{non-invasive field-based methods} to quantify and monitor technical biomechanical movements. In order to improve these methods, the hypothesis of this work is to use a single consumer camera setup accompanied with a pose estimation solution to achieve satisfactory results for recognizing technical gymnastic activities.

The results of the recording process amounts to a total of 96 backflips and 84 back handsprings captured.

\section{Estimating and Extracting Human Action Poses}
\label{estimating-and-collecting-human-action-poses}

The second section of this chapter focuses on the process of extracting human skeleton data from raw video files captured in the first section of this chapter. The skeleton data will be later preprocessed and used to train machine learning algorithms to recognize gymnastic activities.

\subsection{Pose estimation}
\label{pose-estimation}

\begin{comment}
TODO: Add more history of pose estimation
TODO: Add comparison to older skeleton extraction methods
\end{comment}

The author's aim is to approach the problem of extracting skeleton data from raw videos using the most state-of-the-art methods. In recent years, some scientific papers have been released using machine learning based methods for estimating the human pose in images. For example, in 2015 an article was published, where convolutional neural networks were used to estimate human poses in videos \cite{poseestimation2015convnets}. These new methods exceed previous solutions in terms of prediction accuracy.

In recent years, even more complete solutions have been proposed, enabling to estimate human poses in real time thanks to their computational efficiency. For example, a method called \textit{Part Affinity Fields}, essentially a set of vectors encoding the direction of a body part, has been proposed. Every limb is described using an \textit{affinity field} between body parts \cite{DBLP:journals/corr/CaoSWS16}. Such method is used by, for example, an open source solution called \textit{OpenPose} \cite{DBLP:journals/corr/abs-1812-08008}. OpenPose allows to estimate the skeletons of multiple humans from a video recorded with a monocamera. This enables the athlete's skeleton to be identified in an environment with bystanders in the background (a common sight in gymnastic centers).

The author of this work hopes to build a practically usable prototype based on previous work in the pose estimation field. The author uses the free software OpenPose to estimate poses during gymnastic movements. Alternatives to OpenPose performing pose estimation include \textit{PoseNet} \cite{posenet} and \textit{wrnchAI} \cite{wrnchai}.

\subsection{Infrastructure and Tools}

The mandatory requirement for using OpenPose's pose estimation in a reasonable time is a dedicated GPU unit and an access to its general purpose computing API \cite{openpose-requirements-and-dependencies}. The two main API's supported are CUDA for Nvidia GPU and OpenCL for an AMD GPU. Other tools for using OpenPose include CMake for compiling the OpenPose software and also Python programming environment to access OpenPose's API.

A CPU-only setup of OpenPose is also supported, but highly advised by the author of this work to be used only for testing purposes. For software exploration purposes, the author compiled OpenPose on a 2015 MacBook Pro with a \textit{2,7 GHz Dual-Core Intel Core i5} CPU. The time required for pose estimation in a sample recording averaged to around 30 seconds for a single frame. Since using this setup for pose estimation for a total of 180 samples (recorded at 60 FPS) would result in an unreasonable time spent on this process, the author decided to use an Amazon GPU instance with the following basic settings:

\begin{easylist}[itemize]

& Operating system --- Ubuntu Server 18.04 LTS (HVM), SSD Volume Type
& GPU instance --- g4dn.xlarge
& Storage --- 16GB

\end{easylist}

After compiling OpenPose on the Amazon instance, an average pose estimation time of 0.08 seconds was achieved for a single frame in sample recordings.

\subsection{The Pose Estimation Process and Results}
\label{pose-estimation-process-and-results}

\begin{comment}
TODO: Model used
TODO: Total times
\end{comment}

A separate Python module for extracting body key points was developed by the author \cite{openpose-python-pose-extraction-module}. The module uses the Python wrapper of \textit{OpenCV} (open-source computer vision library) to access each frame of each sample video. It then uses the Python wrapper of OpenPose on each frame to estimate poses. Finally, the module dumps extracted data to separate comma separated files.

For each sample recording a separate directory is also created. Each directory contains csv files with the filename pattern \textit{activityPerformed-sampleIndex-subjectName.mov-frameIndex-personIndex}, so for example, a valid filename would be \textit{backflip-1-allar.mov-46-0.csv}. A total of 19119 files were generated from 180 sample recordings. The figure \ref{sample-body-parts-by-openpose} shows an example output dumped into a csv file by the author's written Python OpenPose extraction module. The \textit{X} and \textit{Y} columns mark the corresponding coordinates and the \textit{Confidence score} column represents the confidence factor by OpenPose when estimating body parts. Finally, the \textit{I} column represents the body part index estimated by OpenPose. These indexes match the body part model \textit{BODY\_25} shown in figure \ref{pose-output-format}.

\begin{figure*}
    \centering
\begin{tabular}{c|c|c|c}
    \bfseries I & \bfseries X & \bfseries Y & \bfseries Confidence score % specify table head
    \csvreader[head to column names]{openpose-output-sample.csv}{} % use head of csv as column names
    {\\\hline\i & \x & \y & \c} % specify your coloumns here
\end{tabular}
    \caption{Sample body parts estimated by OpenPose for one frame}
    \label{sample-body-parts-by-openpose}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=10cm]{images/data-acquisition/keypoints-pose-25.png}
    \caption{Pose Output Format --- BODY\_25}
    \label{pose-output-format}
\end{figure*}


The extraction module automatically ran for every sample and generated a total of 19119 csv files. To give it some context, 19119 csv files amount to 5 minutes and 19 seconds of backflips and back handsprings recorded. Using the Amazon GPU instance, the total duration for processing all samples took less than 30 minutes. An estimate could be made for running the same process on a 2015 MacBook Pro with CPU-only setup. The process would take around 6 days and 15 hours if estimated on the basis that the average processing time of each frame is 30 seconds. In conclusion, running the process on an Amazon GPU instance is vaguely more than 300 times faster. This concludes this chapter on data acquisition. The next chapter explores how this data is explored and preprocessed for machine learning training.




















