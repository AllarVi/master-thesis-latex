\section{Implementation overview}

This thesis aims to propose an end-to-end type solution to automate the recognition of gymnastics exercises using computers. High overview of the steps required to achieve this is specified in the following list:

\begin{easylist}[enumerate]
& Data acquisition
    && Human activity recording
    && Estimating and extracting keypoints

& Data exploration
    && Pose estimation limitations

& Data pre-processing
    && Applying pre-processing strategies
    
& Classifier Training
    && Methodology overview
    && Classifier training and validation
    && Analysis of trained classifiers

& Prediction explanations for obtained classifiers
    && Visualizing neuron activations
    && Discussion
    
\end{easylist}

\section{Infrastructure and tools for\\ analysis}

\subsection{Used hardware and software}

There are two main modules in this thesis that require the usage of hardware optimized for the computation of multiple parallel processes. Graphics processing unit (GPU) is used for both pose estimation (section \ref{pose-estimation}) and training classifiers (section \ref{classifier-training-process}). 

A single Amazon EC2 instance with the following specifications is launched as the environment for experimentation:

\begin{easylist}[itemize]

& \textit{Operating system} --- Ubuntu Server 18.04 LTS (Hardware-assisted virtualization - HVM).

& \textit{Instance type} --- g4dn.xlarge (- 4 vCPUs, 2.5 GHz, Intel Xeon P-8259L). 

& \textit{Storage} --- 64GB of general purpose SSD volume type. No specific requirements for storage. The amount was chosen primarily to accommodate the augmented dataset used for training.

& \textit{GPU information} --- NVIDIA T4 Tensor Core GPU \cite{nvidiat4} with 16GB of GDDR6 memory is installed on the g4dn.xlarge Amazon instance. Additionally, NVIDIA 450 Linux driver series with CUDA 10.1 was installed for accessing the GPU unit by Tensorflow.

\end{easylist}

\subsection{Data-processing environment}

All experiments were conducted in the Jupyter Notebook environment (running as a server on the backend instance). The author created separate notebook files for each classifier training, activation analysis, and data exploration. We used Python programming language for developing most of the data processing, classifier training, and validation scripts. Keras, the Python deep learning API, was used with Tensorflow backend for accessing machine learning algorithms. More notable libraries used for experimentation include commonly used \textit{pandas}, \textit{numpy}, \textit{sklearn} and \textit{matplotlib}.

\subsubsection{Additional miscellaneous software}

\begin{easylist}[itemize]

& \textit{FTP server and client} --- FTP server helps with the uploading and downloading of larger datasets. The \textit{vsftpd} package was used in this thesis.

\end{easylist}

