\begin{comment}

    Behind the workings of lime lies the (big) assumption that every complex model is linear on a local scale. While this is not justified in the paper it is not difficult to convince yourself that this is generally sound — you usually expect two very similar observations to behave predictably even in a complex model. 
    
    lime then takes this assumption to its natural conclusion by asserting that it is possible to fit a simple model around a single observation that will mimic how the global model behaves at that locality. The simple model can then be used to explain the predictions of the more complex model locally.
    
    The general approach lime takes to achieving this goal is as follows:
    
    - For each prediction to explain, permute the observation n times.
    - Let the complex model predict the outcome of all permuted observations.
    - Calculate the distance from all permutations to the original observation.
    - Convert the distance to a similarity score.
    - Select m features best describing the complex model outcome from the permuted data.
    - Fit a simple model to the permuted data, explaining the complex model outcome with the m - features from the permuted data weighted by its similarity to the original observation.
    - Extract the feature weights from the simple model and use these as explanations for the complex models local behavior.
    
    It is clear from the above that there’s much wiggle-room in order to optimize the explanation. Chief among the choices that influence the quality of the explanation is how permutations are created, how permutation similarity is calculated, how, and how many, features are selected, and which model is used as the simple model. Some of these choices are hard-coded into lime, while others can be influenced by the user — all of them will be discussed below.
    
    
    
% By “explaining a prediction”, we mean presenting textual or visual artifacts that provide qualitative understanding of the relationship between the instance’s components (e.g. words in text, patches in an image) and the model’s prediction

% In this case, an ex- planation is a small list of symptoms with relative weights – symptoms that either contribute to the prediction (in green) or are evidence against it (in red). Humans usually have prior knowledge about the application domain, which they can use to accept (trust) or reject a prediction if they understand the reasoning behind it.

% An essential criterion for explanations is that they must be interpretable, i.e., provide qualitative understanding between the input variables and the response. 

% Intuitively, an explanation is a local linear approximation of the model's behaviour. While the model may be very complex globally, it is easier to approximate it around the vicinity of a particular instance. While treating the model as a black box, we perturb the instance we want to explain and learn a sparse linear model around it, as an explanation. The figure below illustrates the intuition for this procedure. The model's decision function is represented by the blue/pink background, and is clearly nonlinear. The bright red cross is the instance being explained (let's call it X). We sample instances around X, and weight them according to their proximity to X (weight here is indicated by size). We then learn a linear model (dashed line) that approximates the model well in the vicinity of X, but not necessarily globally. For more information, read our paper, or take a look at this blog post.


\end{comment}

\section{Motivation}

\section{Methodology}

\section{Experimental Result}

